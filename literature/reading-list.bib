

@{document_info,
  author =        {Simon Dobnik},
}

@article{Agrawal:2017aa,
  author =        {Aishwarya Agrawal and Dhruv Batra and Devi Parikh and
                   Aniruddha Kembhavi},
  journal =       {arXiv},
  pages =         {1--15},
  title =         {Don't Just Assume; Look and Answer: Overcoming Priors
                   for Visual Question Answering},
  volume =        {arXiv:1712.00377 [cs.CV]},
  year =          {2017},
  url =           {http://arxiv.org/abs/1712.00377},
}

@mastersthesis{Akram:2017aa,
  address =       {Stockholm, Sweden},
  author =        {Saad Ullah Akram},
  school =        {School of Computer Science and Communication, Control
                   and Robotics, Royal Institute of Technolog},
  title =         {Visual recognition of isolated Swedish sign language
                   signs},
  year =          {2012},
}

@phdthesis{Anderson:2018aa,
  author =        {Peter Anderson},
  month =         {April},
  school =        {The Australian National University},
  type =          {A thesis submitted for the degree of Doctor of
                   Philosophy},
  title =         {Vision and Language Learning: From Image Captioning
                   and Visual Question Answering towards Embodied
                   Agents},
  year =          {2018},
  url =           {https://panderson.me/images/Andersonthesis.pdf},
}

@inproceedings{Anderson:2018ab,
  author =        {Anderson, Peter and He, Xiaodong and Buehler, Chris and
                   Teney, Damien and Johnson, Mark and Gould, Stephen and
                   Zhang, Lei},
  booktitle =     {The IEEE Conference on Computer Vision and Pattern
                   Recognition (CVPR)},
  month =         {June},
  title =         {Bottom-Up and Top-Down Attention for Image Captioning
                   and Visual Question Answering},
  year =          {2018},
  url =           {http://openaccess.thecvf.com/content_cvpr_2018/html/
                  Anderson_Bottom-Up_and_Top-Down_CVPR_2018_paper.html},
}

@article{Andreas:2015aa,
  author =        {Jacob Andreas and Marcus Rohrbach and Trevor Darrell and
                   Dan Klein},
  journal =       {arXiv},
  title =         {Deep Compositional Question Answering with Neural
                   Module Networks},
  volume =        {arXiv:1511.02799 [cs.CV]},
  year =          {2015},
  url =           {http://arxiv.org/abs/1511.02799},
}

@inproceedings{Andreas:2016aa,
  address =       {San Diego, California},
  author =        {Jacob Andreas and Marcus Rohrbach and Trevor Darrell and
                   Dan Klein},
  booktitle =     {Proceedings of {NAACL-HLT} 2016},
  journal =       {CoRR},
  month =         {June 12-17},
  organization =  {Association for Computational Linguistics},
  pages =         {1545--1554},
  title =         {Learning to Compose Neural Networks for Question
                   Answering},
  year =          {2016},
  url =           {http://arxiv.org/abs/1601.01705},
}

@techreport{Astbom:2017aa,
  address =       {Gothenburg, Sweden},
  author =        {{\AA}stbom, Amelie},
  institution =   {Department of Philosophy, Linguistics and Theory of
                   Science (FLOV), University of Gothenburg},
  month =         {February 7},
  note =          {{S}upervisor: Simon Dobnik, opponent: Linnea Strand,
                   examiner: Christine Howes},
  type =          {C-uppsats (Bachelor's thesis/extended essay)},
  title =         {How function of objects affects geometry of spatial
                   descriptions. {A} study of {Swedish} and {Japanese}},
  year =          {2017},
}

@article{Baron-Cohen:1985aa,
  author =        {Simon Baron-Cohen and Alan M. Leslie and Uta Frith},
  journal =       {Cognition},
  month =         {October},
  number =        {1},
  pages =         {37--46},
  title =         {Does the autistic child have a ``theory of mind'' ?},
  volume =        {21},
  year =          {1985},
  url =           {https://doi.org/10.1016/0010-0277(85)90022-8},
}

@article{Barsalou:1999uq,
  author =        {Lawrence W. Barsalou},
  journal =       {Behavioral and Brain Sciences},
  pages =         {577--609},
  title =         {Perceptual symbol systems},
  volume =        {22},
  year =          {1999},
}

@article{Barsalou:2008aa,
  author =        {Barsalou, Lawrence W.},
  journal =       {Annual Review of Psychology},
  pages =         {617--645},
  title =         {Grounded cognition},
  volume =        {59},
  year =          {2008},
  abstract =      {Grounded cognition rejects traditional views that
                   cognition is computation on amodal symbols in a
                   modular system, independent of the brain's modal
                   systems for perception, action, and introspection.
                   Instead, grounded cognition proposes that modal
                   simulations, bodily states, and situated action
                   underlie cognition. Accumulating behavioral and
                   neural evidence supporting this view is reviewed from
                   research on perception, memory, knowledge, language,
                   thought, social cognition, and development. Theories
                   of grounded cognition are also reviewed, as are
                   origins of the area and common misperceptions of it.
                   Theoretical, empirical, and methodological issues are
                   raised whose future treatment is likely to affect the
                   growth and impact of grounded cognition.},
  doi =           {10.1146/annurev.psych.59.103006.093639},
  url =           {https://doi.org/10.1146/annurev.psych.59.103006.093639},
}

@article{Bateman:2019aa,
  author =        {Bateman, John A. and Pomarlan, Mihai and
                   Kazhoyan, Gayane},
  journal =       {Applied Ontology},
  month =         {2 October},
  pages =         {1--35},
  title =         {Embodied contextualization: Towards a multistratal
                   ontological treatment},
  volume =        {Pre-press},
  year =          {2019},
  doi =           {10.3233/AO-190218},
  url =           {https://content.iospress.com/articles/applied-ontology/
                  ao190218},
}

@article{Battaglia:2013aa,
  author =        {Battaglia, Peter W. and Hamrick, Jessica B. and
                   Tenenbaum, Joshua B.},
  journal =       {Proceedings of the National Academy of Sciences},
  number =        {45},
  pages =         {18327-18332},
  title =         {Simulation as an engine of physical scene
                   understanding},
  volume =        {110},
  year =          {2013},
  abstract =      {In a glance, we can perceive whether a stack of
                   dishes will topple, a branch will support a child's
                   weight, a grocery bag is poorly packed and liable to
                   tear or crush its contents, or a tool is firmly
                   attached to a table or free to be lifted. Such rapid
                   physical inferences are central to how people
                   interact with the world and with each other, yet
                   their computational underpinnings are poorly
                   understood. We propose a model based on an
                   ``intuitive physics engine,'' a cognitive mechanism
                   similar to computer engines that simulate rich
                   physics in video games and graphics, but that uses
                   approximate, probabilistic simulations to make robust
                   and fast inferences in complex natural scenes where
                   crucial information is unobserved. This single model
                   fits data from five distinct psychophysical tasks,
                   captures several illusions and biases, and explains
                   core aspects of human mental models and common-sense
                   reasoning that are instrumental to how humans
                   understand their everyday world.},
  doi =           {10.1073/pnas.1306572110},
  url =           {http://www.pnas.org/content/110/45/18327.abstract},
}

@article{Bernardi:2016aa,
  author =        {Bernardi, Raffaella and Cakici, Ruket and
                   Elliott, Desmond and Erdem, Aykut and Erdem, Erkut and
                   Ikizler-Cinbis, Nazli and Keller, Frank and
                   Muscat, Adrian and Plank, Barbara},
  journal =       {Journal of Artificial Intelligence Research},
  pages =         {409--442},
  title =         {Automatic description generation from images: A
                   survey of models, datasets, and evaluation measures},
  volume =        {55},
  year =          {2016},
  doi =           {https://doi.org/10.1613/jair.4900},
}

@book{Bradski:2008aa,
  author =        {Bradski, Gary and Kaehler, Adrian},
  publisher =     {O'Reilly Media, Inc.},
  title =         {Learning {OpenCV}: Computer vision with the {OpenCV}
                   library},
  year =          {2008},
}

@article{Breazeal:2006aa,
  author =        {Cynthia Breazeal and Matt Berlin and Andrew Brooks and
                   Jesse Gray and Andrea L. Thomaz},
  journal =       {Robotics and Autonomous Systems},
  number =        {5},
  pages =         {385--393},
  title =         {Using perspective taking to learn from ambiguous
                   demonstrations},
  volume =        {54},
  year =          {2006},
  abstract =      {This paper addresses an important issue in learning
                   from demonstrations that are provided by
                   ``na{\"\i}ve'' human teachers---people who do not
                   have expertise in the machine learning algorithms
                   used by the robot. We therefore entertain the
                   possibility that, whereas the average human user may
                   provide sensible demonstrations from a human's
                   perspective, these same demonstrations may be
                   insufficient, incomplete, ambiguous, or otherwise
                   ``flawed'' from the perspective of the training set
                   needed by the learning algorithm to generalize
                   properly. To address this issue, we present a system
                   where the robot is modeled as a socially engaged and
                   socially cognitive learner. We illustrate the merits
                   of this approach through an example where the robot
                   is able to correctly learn from ``flawed''
                   demonstrations by taking the visual perspective of
                   the human instructor to clarify potential
                   ambiguities.},
  doi =           {http://dx.doi.org/10.1016/j.robot.2006.02.004},
  issn =          {0921-8890},
}

@article{Bruni:2014aa,
  author =        {Bruni, Elia and Tran, Nam-Khanh and Baroni, Marco},
  journal =       {Journal of Artificial Intelligence Research (JAIR)},
  number =        {1--47},
  title =         {Multimodal Distributional Semantics},
  volume =        {49},
  year =          {2014},
}

@inproceedings{Byron:2003aa,
  author =        {Byron, Donna K},
  booktitle =     {Proceedings of the First International Workshop on
                   Language Understanding and Agents for Real World
                   Interaction},
  pages =         {39--47},
  title =         {Understanding referring expressions in situated
                   language some challenges for real-world agents},
  year =          {2003},
}

@inproceedings{Can:2019aa,
  address =       {Minneapolis, Minnesota},
  author =        {Can, Ozan Arkan and Zuidberg Dos Martires, Pedro and
                   Persson, Andreas and Gaal, Julian and Loutfi, Amy and
                   De Raedt, Luc and Yuret, Deniz and
                   Saffiotti, Alessandro},
  booktitle =     {Proceedings of the Combined Workshop on Spatial
                   Language Understanding ({S}p{LU}) and Grounded
                   Communication for Robotics ({R}obo{NLP})},
  month =         jun,
  pages =         {29--39},
  publisher =     {Association for Computational Linguistics},
  title =         {Learning from Implicit Information in Natural
                   Language Instructions for Robotic Manipulations},
  year =          {2019},
  abstract =      {Human-robot interaction often occurs in the form of
                   instructions given from a human to a robot. For a
                   robot to successfully follow instructions, a common
                   representation of the world and objects in it should
                   be shared between humans and the robot so that the
                   instructions can be grounded. Achieving this
                   representation can be done via learning, where both
                   the world representation and the language grounding
                   are learned simultaneously. However, in robotics this
                   can be a difficult task due to the cost and scarcity
                   of data. In this paper, we tackle the problem by
                   separately learning the world representation of the
                   robot and the language grounding. While this approach
                   can address the challenges in getting sufficient
                   data, it may give rise to inconsistencies between
                   both learned components. Therefore, we further
                   propose Bayesian learning to resolve such
                   inconsistencies between the natural language
                   grounding and a robot{'}s world representation by
                   exploiting spatio-relational information that is
                   implicitly present in instructions given by a human.
                   Moreover, we demonstrate the feasibility of our
                   approach on a scenario involving a robotic arm in the
                   physical world.},
  url =           {https://www.aclweb.org/anthology/W19-1604},
}

@article{Cangelosi:2010aa,
  author =        {A. {Cangelosi} and G. {Metta} and G. {Sagerer} and
                   S. {Nolfi} and C. {Nehaniv} and K. {Fischer} and
                   J. {Tani} and T. {Belpaeme} and G. {Sandini} and
                   F. {Nori} and L. {Fadiga} and B. {Wrede} and
                   K. {Rohlfing} and E. {Tuci} and K. {Dautenhahn} and
                   J. {Saunders} and A. {Zeschel}},
  journal =       {IEEE Transactions on Autonomous Mental Development},
  month =         {September},
  number =        {3},
  pages =         {167-195},
  title =         {Integration of Action and Language Knowledge: A
                   Roadmap for Developmental Robotics},
  volume =        {2},
  year =          {2010},
  abstract =      {This position paper proposes that the study of
                   embodied cognitive agents, such as humanoid robots,
                   can advance our understanding of the cognitive
                   development of complex sensorimotor, linguistic, and
                   social learning skills. This in turn will benefit the
                   design of cognitive robots capable of learning to
                   handle and manipulate objects and tools autonomously,
                   to cooperate and communicate with other robots and
                   humans, and to adapt their abilities to changing
                   internal, environmental, and social conditions. Four
                   key areas of research challenges are discussed,
                   specifically for the issues related to the
                   understanding of: 1) how agents learn and represent
                   compositional actions; 2) how agents learn and
                   represent compositional lexica; 3) the dynamics of
                   social interaction and learning; and 4) how
                   compositional action and language representations are
                   integrated to bootstrap the cognitive system. The
                   review of specific issues and progress in these areas
                   is then translated into a practical roadmap based on
                   a series of milestones. These milestones provide a
                   possible set of cognitive robotics goals and test
                   scenarios, thus acting as a research roadmap for
                   future work on cognitive developmental robotics.},
  doi =           {10.1109/TAMD.2010.2053034},
  issn =          {1943-0604},
}

@inproceedings{Cano-Santin:2019aa,
  address =       {London, UK},
  author =        {Cano Sant{\'\i}n, Jos{\'e} Miguel and Dobnik, Simon and
                   Ghanimifard, Mehdi},
  booktitle =     {Proceedings of {LondonLogue} -- {Semdial} 2019: The
                   23rd Workshop on the Semantics and Pragmatics of
                   Dialogue},
  editor =        {Hough, Julian and Howes, Christine and
                   Kennington, Casey},
  month =         {4--6 September},
  organization =  {Queen Mary University of London},
  pages =         {1--3},
  title =         {Interactive visual grounding with neural networks},
  year =          {2019},
  abstract =      {Training strategies for neural networks are not
                   suitable for real time human-robot interaction.
                   Few-shot learning approaches have been developed for
                   low resource scenarios but without the usual
                   teacher/learner supervision. In this work we present
                   a combination of both: a situated dialogue system to
                   teach object names to a robot from its camera images
                   using Matching Networks (Vinyals et al., 2016). We
                   compare the performance of the system with
                   transferred learning from pre-trained models and
                   different conversational strategies with a human
                   tutor. grounding, object learning, interactive
                   learning, transfer learning, neural networks},
  url =           {https://gup.ub.gu.se/publication/283004?lang=en},
}

@techreport{Cano-Santin:2019ab,
  address =       {Gothenburg, Sweden},
  author =        {Cano Sant{\'\i}n, Jos{\'e} Miguel},
  institution =   {Department of Philosophy, Linguistics and Theory of
                   Science (FLOV), University of Gothenburg},
  month =         {September 18},
  note =          {{S}upervisor: Simon Dobnik and Mehdi Ghanimifard,
                   examiner: Aarne Ranta},
  type =          {Masters in Language Technology (MLT), 30 HEC},
  title =         {Fast visual grounding in interaction: bringing
                   few-shot learning with neural networks to an
                   interactive robot},
  year =          {2019},
  url =           {http://hdl.handle.net/2077/62035},
}

@article{Cao:2018aa,
  author =        {Kris Cao and Angeliki Lazaridou and Marc Lanctot and
                   Joel Z Leibo and Karl Tuyls and Stephen Clark},
  journal =       {arXiv},
  title =         {Emergent Communication through Negotiation},
  volume =        {arXiv:1804.03980 [cs.AI]},
  year =          {2018},
  url =           {https://arxiv.org/abs/1804.03980},
}

@incollection{Chai:2018aa,
  author =        {Chai, Joyce Y and Cakmak, Maya and Sidner, Candace},
  booktitle =     {Interactive Task Learning: Agents, Robots, and Humans
                   Acquiring New Tasks through Natural Interactions},
  chapter =       {9},
  editor =        {K. A. Cluck and J. E. Laird},
  publisher =     {MIT press},
  series =        {Strungmann Forum Reports},
  title =         {Teaching Robots New Tasks through Natural
                   Interaction},
  year =          {2018},
}

@inproceedings{Chai:2018ab,
  author =        {Joyce Y. Chai and Qiaozi Gao and Lanbo She and
                   Shaohua Yang and Sari Saba-Sadiya and Guangyue Xu},
  booktitle =     {Proceedings of the Twenty-Seventh International Joint
                   Conference on Artificial Intelligence, {IJCAI-18}},
  month =         {7},
  pages =         {2--9},
  publisher =     {International Joint Conferences on Artificial
                   Intelligence Organization},
  title =         {Language to Action: Towards Interactive Task Learning
                   with Physical Agents},
  year =          {2018},
  doi =           {10.24963/ijcai.2018/1},
  url =           {https://doi.org/10.24963/ijcai.2018/1},
}

@article{Collell:2017aa,
  author =        {Guillem Collell and Luc Van Gool and
                   Marie{-}Francine Moens},
  journal =       {arXiv},
  pages =         {1--8},
  title =         {Acquiring Common Sense Spatial Knowledge through
                   Implicit Spatial Templates},
  volume =        {arXiv:1711.06821 [cs.AI]},
  year =          {2017},
  bibsource =     {dblp computer science bibliography, https://dblp.org},
  url =           {http://arxiv.org/abs/1711.06821},
}

@article{Collell:2018aa,
  author =        {Collell, Guillem and Moens, Marie-Francine},
  journal =       {Transactions of the Association for Computational
                   Linguistics},
  pages =         {133--144},
  title =         {Learning Representations Specialized in Spatial
                   Knowledge: Leveraging Language and Vision},
  volume =        {6},
  year =          {2018},
  abstract =      {Spatial understanding is crucial in many real-world
                   problems, yet little progress has been made towards
                   building representations that capture spatial
                   knowledge. Here, we move one step forward in this
                   direction and learn such representations by
                   leveraging a task consisting in predicting continuous
                   2D spatial arrangements of objects given
                   object-relationship-object instances (e.g., ``cat
                   under chair\") and a simple neural network model that
                   learns the task from annotated images. We show that
                   the model succeeds in this task and that it is
                   furthermore capable of predicting correct spatial
                   arrangements for unseen objects if either CNN
                   features or word embeddings of the objects are
                   provided. The differences between visual and
                   linguistic features are discussed. Next, to evaluate
                   the spatial representations learned in the previous
                   task, we introduce a task and a dataset consisting in
                   a set of crowdsourced human ratings of spatial
                   similarity for object pairs. We find that both CNN
                   features and word embeddings predict well human
                   judgments of similarity and that these vectors can be
                   further specialized in spatial knowledge if we update
                   them when training the model that predicts spatial
                   arrangements of objects. Overall, this paper paves
                   the way towards building distributed spatial
                   representations, contributing to the understanding of
                   spatial expressions in language.},
  issn =          {2307-387X},
  url =           {https://www.transacl.org/ojs/index.php/tacl/article/view/
                  1214},
}

@article{Collell:2018ab,
  author =        {Guillem Collell and Marie-Francine Moens},
  journal =       {arXiv},
  pages =         {1--13},
  title =         {Do Neural Network Cross-Modal Mappings Really Bridge
                   Modalities?},
  volume =        {arXiv:1805.07616 [stat.ML]},
  year =          {2018},
}

@unpublished{Cooperinprepa,
  author =        {Robin Cooper},
  note =          {Draft at
  \url{https://sites.google.com/site/typetheorywithrecords/drafts}},
  title =         {From perception to communication: An analysis of
                   meaning and action using a theory of types with
                   records {(TTR)}},
  year =          {in prep},
  url =           {https://sites.google.com/site/typetheorywithrecords/drafts},
}

@incollection{Coventry:2005aa,
  author =        {Coventry, Kenny R. and Cangelosi, Angelo and
                   Rajapakse, Rohanna and Bacon, Alison and
                   Newstead, Stephen and Joyce, Dan and
                   Richards, Lynn V.},
  booktitle =     {Spatial Cognition IV. Reasoning, Action, Interaction},
  editor =        {Freksa, Christian and Knauff, Markus and
                   Krieg-Br{\"u}ckner, Bernd and Nebel, Bernhard and
                   Barkowsky, Thomas},
  pages =         {98-110},
  publisher =     {Springer Berlin Heidelberg},
  series =        {Lecture Notes in Computer Science},
  title =         {Spatial Prepositions and Vague Quantifiers:
                   Implementing the Functional Geometric Framework},
  volume =        {3343},
  year =          {2005},
  doi =           {10.1007/978-3-540-32255-9\_6},
  isbn =          {978-3-540-25048-7},
}

@incollection{Coventry:2005ab,
  author =        {Coventry, Kenny and Garrod, Simon},
  booktitle =     {Functional features in language and space: insights
                   from perception, categorization, and development},
  editor =        {Carlson, Laura Anne and Zee, Emile van der},
  pages =         {149--162},
  publisher =     {Oxford University Press},
  title =         {Spatial prepositions and the functional geometric
                   framework. Towards a classification of
                   extra-geometric influences.},
  volume =        {2},
  year =          {2005},
}

@inproceedings{Das:2017aa,
  author =        {Das, Abhishek and Kottur, Satwik and Gupta, Khushi and
                   Singh, Avi and Yadav, Deshraj and Moura, Jos{\'e} MF and
                   Parikh, Devi and Batra, Dhruv},
  booktitle =     {Proceedings of the IEEE Conference on Computer Vision
                   and Pattern Recognition},
  pages =         {326--335},
  title =         {Visual dialog},
  year =          {2017},
}

@inproceedings{De-Vries:2017aa,
  author =        {De Vries, Harm and Strub, Florian and Chandar, Sarath and
                   Pietquin, Olivier and Larochelle, Hugo and
                   Courville, Aaron},
  booktitle =     {Proceedings of the IEEE Conference on Computer Vision
                   and Pattern Recognition},
  month =         {July},
  pages =         {5503--5512},
  title =         {Guesswhat?! visual object discovery through
                   multi-modal dialogue},
  year =          {2017},
}

@inproceedings{Demirel:2017aa,
  author =        {Demirel, Berkan and Gokberk Cinbis, Ramazan and
                   Ikizler-Cinbis, Nazli},
  booktitle =     {The IEEE International Conference on Computer Vision
                   (ICCV)},
  month =         {October},
  title =         {Attributes2Classname: A Discriminative Model for
                   Attribute-Based Unsupervised Zero-Shot Learning},
  year =          {2017},
}

@article{Dissanayake:2001,
  author =        {Dissanayake, M. W. M. G and Newman, P. M. and
                   Durrant-Whyte, H. F. and Clark, S. and Csorba, M.},
  journal =       {IEEE Transactions on Robotic and Automation},
  number =        {3},
  pages =         {229-241},
  title =         {A solution to the simultaneous localization and map
                   building ({SLAM}) problem},
  volume =        {17},
  year =          {2001},
}

@phdthesis{Dobnik:2009dz,
  address =       {Oxford, United Kingdom},
  author =        {Dobnik, Simon},
  month =         {September 4},
  school =        {University of Oxford: Faculty of Linguistics,
                   Philology and Phonetics and The Queen's College},
  title =         {Teaching mobile robots to use spatial words},
  year =          {2009},
  url =           {https://gup.ub.gu.se/publication/270997},
}

@inproceedings{Dobnik:2013aa,
  address =       {Berlin, Germany},
  author =        {Dobnik, Simon and Kelleher, John D.},
  booktitle =     {Proceedings of PRE-CogSsci 2013: Production of
                   referring expressions -- bridging the gap between
                   cognitive and computational approaches to reference},
  month =         {31 July},
  pages =         {1--6},
  title =         {Towards an automatic identification of functional and
                   geometric spatial prepositions},
  year =          {2013},
  url =           {http://pre2013.uvt.nl/pdf/dobnik-kelleher.pdf},
}

@inproceedings{Dobnik:2014aa,
  address =       {Edinburgh},
  author =        {Dobnik, Simon and Kelleher, John D. and
                   Koniaris, Christos},
  booktitle =     {Proceedings of {DialWatt} -- Semdial 2014: The 18th
                   Workshop on the Semantics and Pragmatics of Dialogue},
  editor =        {Verena Rieser and Philippe Muller},
  month =         {1--3 September},
  pages =         {43--52},
  title =         {Priming and Alignment of Frame of Reference in
                   Situated Conversation},
  year =          {2014},
  url =           {http://gup.ub.gu.se/records/fulltext/200741/200741.pdf},
}

@inproceedings{Dobnik:2014ab,
  address =       {Dublin, Ireland},
  author =        {Dobnik, Simon and Kelleher, John D.},
  booktitle =     {Proceedings of the Third V\&L Net Workshop on Vision
                   and Language},
  month =         {August},
  pages =         {33--37},
  publisher =     {Dublin City University and the Association for
                   Computational Linguistics},
  title =         {Exploration of functional semantics of prepositions
                   from corpora of descriptions of visual scenes},
  year =          {2014},
  url =           {http://www.aclweb.org/anthology/W14-5405},
}

@inproceedings{Dobnik:2015aa,
  address =       {Gothenburg, Sweden},
  author =        {Dobnik, Simon and Howes, Christine and
                   Kelleher, John D.},
  booktitle =     {Proceedings of {goDIAL} -- Semdial 2015: The 19th
                   Workshop on the Semantics and Pragmatics of Dialogue},
  editor =        {Howes, Christine and Larsson, Staffan},
  month =         {24--26th August},
  pages =         {24--32},
  title =         {Changing perspective: Local alignment of reference
                   frames in dialogue},
  year =          {2015},
  abstract =      {In this paper we examine how people negotiate,
                   interpret and repair the frame of reference (FoR) in
                   free dialogues discussing spatial scenes. We describe
                   a pilot study in which participants are given
                   different perspectives of the same scene and asked to
                   locate several objects that are only shown on one of
                   their pictures. This task requires participants to
                   coordinate on FoR in order to identify the missing
                   objects. Preliminary results indicate that
                   conversational participants align locally on FoR but
                   do not converge on a global frame of reference.
                   Misunderstandings lead to clarification sequences in
                   which participants shift the FoR. These findings have
                   implications for situated dialogue systems.},
  url =           {https://gup.ub.gu.se/publication/224188},
}

@inproceedings{Dobnik:2017aa,
  address =       {Gothenburg, Sweden},
  author =        {Dobnik, Simon and de Graaf, Erik},
  booktitle =     {Proceedings of the 21st Nordic Conference on
                   Computational Linguistics (NoDaLiDa)},
  editor =        {Tiedemann, J{\"o}rg and Tahmasebi, Nina},
  month =         {22--24 May},
  organization =  {Northern European Association for Language Technology
                   (NEALT)},
  pages =         {162--171},
  publisher =     {Association for Computational Linguistics},
  title =         {{KILLE}: a Framework for Situated Agents for Learning
                   Language Through Interaction},
  year =          {2017},
  url =           {https://gup.ub.gu.se/publication/253950},
}

@inproceedings{Dobnik:2017ac,
  address =       {Saarbr{\"u}cken, Germany},
  author =        {Dobnik, Simon and {\AA}stbom, Amelie},
  booktitle =     {Proceedings of {Saardial} -- Semdial 2017: The 21st
                   Workshop on the Semantics and Pragmatics of Dialogue},
  editor =        {Petukhova, Volha and Tian, Ye},
  month =         {August 15--17},
  pages =         {17--26},
  title =         {{(Perceptual)} grounding as interaction},
  year =          {2017},
  url =           {https://gup.ub.gu.se/publication/255455},
}

@inproceedings{Dobnik:2018ab,
  address =       {New Orleans, Louisiana, USA},
  author =        {Dobnik, Simon and Ghanimifard, Mehdi and
                   Kelleher, John D.},
  booktitle =     {Proceedings of the First International Workshop on
                   Spatial Language Understanding {(SpLU 2018)} at
                   {NAACL-HLT 2018}},
  month =         {June 6},
  organization =  {Association for Computational Linguistics},
  pages =         {1--11},
  title =         {Exploring the Functional and Geometric Bias of
                   Spatial Relations Using Neural Language Models},
  year =          {2018},
  abstract =      {The challenge for computational models of spatial
                   descriptions for situated dialogue systems is the
                   integration of information from different modalities.
                   The semantics of spatial descriptions are grounded in
                   at least two sources of information: (i) a geometric
                   representation of space and (ii) the functional
                   interaction of related objects that. We train several
                   neural language models on descriptions of scenes from
                   a dataset of image captions and examine whether the
                   functional or geometric bias of spatial descriptions
                   reported in the literature is reflected in the
                   estimated perplexity of these models. The results of
                   these experiments have implications for the creation
                   of models of spatial lexical semantics for
                   human-robot dialogue systems. Furthermore, they also
                   provide an insight into the kinds of the semantic
                   knowledge captured by neural language models trained
                   on spatial descriptions, which has implications for
                   image captioning systems.},
  url =           {https://gup.ub.gu.se/publication/267064?lang=en},
}

@techreport{Dobnik:2019ae,
  address =       {University of Latvia, Riga, Latvia},
  author =        {Dobnik, Simon and Kelleher, John D. and
                   Ghanimifard, Mehdi},
  institution =   {ESSLLI 2019, 31 European Summer School on Logic,
                   Language and Information},
  month =         {12--16 August},
  type =          {lecture notes},
  title =         {Language, Action and Perception ({APL-ESSLLI}):
                   Lecture Notes of a Course in Language and
                   Computation},
  year =          {2019},
  abstract =      {The course gives a survey of theory and practical
                   computational implementations of how natural language
                   interacts with the physical world through action and
                   perception. We will look at topics such as semantic
                   theories and computational approaches to modelling
                   natural language, action and perception (grounding),
                   situated dialogue systems, integrated robotic
                   systems, grounding of language in action and
                   perception, generation and interpretation of scene
                   descriptions from images and videos, spatial
                   cognition, and others. language, perception, action,
                   embodiment, spatial language, attention, grounding,
                   dialogue systems, robotic systems, learning language
                   with robots, image descriptions, generating referring
                   expressions, visual question answering, situated
                   interactive agents},
  url =           {https://gup.ub.gu.se/publication/283008?lang=en},
}

@article{Dogan:2019aa,
  author =        {Fethiye Irmak Do{\u g}an and Sinan Kalkan and
                   Iolanda Leite},
  journal =       {arXiv},
  pages =         {1--8},
  title =         {Learning to Generate Unambiguous Spatial Referring
                   Expressions for Real-World Environments},
  volume =        {arXiv:1904.07165 [cs.RO]},
  year =          {2019},
}

@inproceedings{Elliott:2018aa,
  address =       {Brussels, Belgium},
  author =        {Elliott, Desmond},
  booktitle =     {Proceedings of the 2018 Conference on Empirical
                   Methods in Natural Language Processing},
  month =         {October-November},
  pages =         {2974--2978},
  publisher =     {Association for Computational Linguistics},
  title =         {Adversarial Evaluation of Multimodal Machine
                   Translation},
  year =          {2018},
  url =           {http://www.aclweb.org/anthology/D18-1329},
}

@article{Foerster:2016aa,
  author =        {Jakob N. Foerster and Yannis M. Assael and
                   Nando de Freitas and Shimon Whiteson},
  journal =       {CoRR},
  title =         {Learning to Communicate with Deep Multi-Agent
                   Reinforcement Learning},
  volume =        {abs/1605.06676},
  year =          {2016},
}

@inproceedings{Forestier:2017aa,
  author =        {Forestier, S{\'e}bastien and Oudeyer, Pierre-Yves},
  booktitle =     {39th Annual Conference of the Cognitive Science
                   Society (CogSci 2017)},
  pages =         {2013--2018},
  title =         {A unified model of speech and tool use early
                   development},
  year =          {2017},
}

@inproceedings{Ghanimifard:2017ab,
  address =       {Montpellier, France},
  author =        {Ghanimifard, Mehdi and Dobnik, Simon},
  booktitle =     {Proceedings of {IWCS 2017}: 12th International
                   Conference on Computational Semantics},
  editor =        {Gardent, Claire and Retor\'{e}, Christian},
  month =         {September 19--22},
  pages =         {1--12},
  publisher =     {Association for Computational Linguistics},
  title =         {Learning to Compose Spatial Relations with Grounded
                   Neural Language Models},
  year =          {2017},
  abstract =      {Language is compositional: we can generate and
                   interpret novel sentences by having a notion of the
                   meaning of their individual parts. Spatial
                   descriptions are grounded in perceptional
                   representations but their meaning is also defined by
                   what neighbouring words they co-occur with. In this
                   paper, we examine how language models conditioned on
                   perceptual features can capture the semantics of
                   composed phrases as well as of individual words. We
                   generate a synthetic dataset of spatial descriptions
                   referring to perceptual scenes and examine how
                   grounded language models built with deep neural
                   networks can account for compositionality of
                   descriptions -- by evaluating how the learned
                   language models can deal with novel grounded composed
                   descriptions and novel grounded decomposed
                   descriptions, constituents previously not seen in
                   isolation.},
  url =           {https://gup.ub.gu.se/publication/257763?lang=en},
}

@inproceedings{Ghanimifard:2018ab,
  address =       {Proceedings of the Workshop on Shortcomings in Vision
                   and Language (SiVL), ECCV 2018, Munich, Germany},
  author =        {Ghanimifard, Mehdi and Dobnik, Simon},
  booktitle =     {Computer Vision -- ECCV 2018 Workshops. ECCV 2018},
  editor =        {Leal-Taix{\'e}, L. and Roth, S.},
  pages =         {1--9},
  publisher =     {Springer, Cham},
  series =        {Lecture Notes in Computer Science (LNCS)},
  title =         {Knowing When to Look For What and Where: Evaluating
                   Generation of Spatial Descriptions with Adaptive
                   Attention},
  volume =        {11132},
  year =          {2018},
  doi =           {10.1007/978-3-030-11018-5_14},
  url =           {https://gup.ub.gu.se/publication/274350?lang=en},
}

@inproceedings{Ghanimifard:2019aa,
  address =       {Minneapolis, Minnesota, USA},
  author =        {Ghanimifard, Mehdi and Dobnik, Simon},
  booktitle =     {Proceedings of the Combined Workshop on Spatial
                   Language Understanding (SpLU) and Grounded
                   Communication for Robotics (RoboNLP)},
  editor =        {Archna Bhatia and Yonatan Bisk and
                   Parisa Kordjamshidi and Jesse Thomason},
  month =         {6 June},
  organization =  {North American Chapter of the Association for
                   Computational Linguistics: Human Language
                   Technologies (NAACL-HLT 2019)},
  pages =         {71--81},
  publisher =     {Association for Computational Linguistics},
  title =         {{What} a neural language model tells us about spatial
                   relations},
  year =          {2019},
  abstract =      {Understanding and generating spatial descriptions
                   requires knowledge about what objects are related,
                   their functional interactions, and where the objects
                   are geometrically located. Different spatial
                   relations have different functional and geometric
                   bias. The wide usage of neural language models in
                   different areas including generation of image
                   description motivates the study of what kind of
                   knowledge is encoded in neural language models about
                   individual spatial relations. With the premise that
                   the functional bias of relations is expressed in
                   their word distributions, we construct multi-word
                   distributional vector representations and show that
                   these representations perform well on intrinsic
                   semantic reasoning tasks, thus confirming our
                   premise. A comparison of our vector representations
                   to human semantic judgments indicates that different
                   bias (functional or geometric) is captured in
                   different data collection tasks which suggests that
                   the contribution of the two meaning modalities is
                   dynamic, related to the context of the task. spatial
                   description, spatial language, neural language model,
                   vector representation, spatial reasoning},
  url =           {https://gup.ub.gu.se/publication/279635?lang=en},
}

@inproceedings{Ghanimifard:2019ab,
  address =       {Tokyo, Japan},
  author =        {Ghanimifard, Mehdi and Dobnik, Simon},
  booktitle =     {Proceedings of the 12th International Conference on
                   Natural Language Generation (INLG-2019)},
  editor =        {van Deemter, Kees and Lin, Chenghua and
                   Takamura, Hiroya},
  month =         {29 October -- 1 November},
  organization =  {Association for Computational Linguistics},
  pages =         {1--15},
  title =         {What goes into a word: generating image descriptions
                   with top-down spatial knowledge},
  year =          {2019},
  abstract =      {Generating grounded image descriptions requires
                   associating linguistic units with their corresponding
                   visual clues. A common method is to train a decoder
                   language model with attention mechanism over
                   convolutional visual features. Attention weights
                   align the stratified visual features arranged by
                   their location with tokens, most commonly words, in
                   the target description. However, words such as
                   spatial relations (e.g. next to and under) are not
                   directly referring to geometric arrangements of
                   pixels but to complex geometric and conceptual
                   representations. The aim of this paper is to evaluate
                   what representations facilitate generating image
                   descriptions with spatial relations and lead to
                   better grounded language generation. In particular,
                   we investigate the contribution of four different
                   representational modalities in generating relational
                   referring expressions: (i) (pre-trained)
                   convolutional visual features, (ii) spatial attention
                   over visual features, (iii) top-down geometric
                   relational knowledge between objects, and (iv) world
                   knowledge captured by contextual embeddings in
                   language models. spatial descriptions grounded neural
                   language models attention representation learning},
  url =           {https://gup.ub.gu.se/publication/284052?lang=en},
}

@article{Glenberg:1997aa,
  author =        {Glenberg, Arthur M.},
  journal =       {The Behavioral and brain sciences},
  month =         {April},
  pages =         {1--55},
  title =         {What memory is for},
  volume =        {20},
  year =          {1997},
}

@book{Goebel:2013aa,
  author =        {Patrick Goebel},
  publisher =     {Lulu},
  title =         {{ROS} by example},
  year =          {2013},
}

@inproceedings{Goyal:2017aa,
  author =        {Yash Goyal and Tejas Khot and Douglas Summers{-}Stay and
                   Dhruv Batra and Devi Parikh},
  booktitle =     {Conference on Computer Vision and Pattern Recognition
                   (CVPR)},
  pages =         {1--11},
  title =         {Making the {V} in {VQA} Matter: Elevating the Role of
                   Image Understanding in {V}isual {Q}uestion
                   {A}nswering},
  year =          {2017},
}

@mastersthesis{Graaf:2016aa,
  address =       {Gothenburg, Sweden},
  author =        {de Graaf, Erik},
  month =         {June, 8th},
  note =          {{S}upervisor: Simon Dobnik, examiner: Richard
                   Johansson, opponent: Lorena Llozhi},
  school =        {Department of Philosophy, Linguistics and Theory of
                   Science. University of Gothenburg},
  title =         {Learning Objects and Spatial Relations with {K}inect},
  year =          {2016},
  abstract =      {In order for humans to have meaningful interactions
                   with a robotic system, this system should be capable
                   of grounding semantic representations to their
                   real-world representations, learn spatial
                   relationships and communicate using spoken human
                   language. End users need to be able to query the
                   system what objects it already has knowledge of, for
                   more efficient learning. Such systems exist, but
                   require large sample sizes, thus not allowing end
                   users to teach the system more objects when needed.
                   To overcome this problem, we developed a non-mobile
                   system dubbed Kille, that uses a 3D camera, SIFT
                   features and machine learning to allow a tutor to
                   teach the system objects and spatial relations. The
                   system is built upon the ROS (Robot Operating System)
                   framework and uses Opendial software as a dialogue
                   system, for which a ROS support was written as part
                   of this project. We describe the hardware of the
                   system, the software used and developed, and we
                   evaluate its performance. Our results show that Kille
                   performs well on small learning sets, considering the
                   low sample size it uses to learn. In contrast to
                   other approaches, we focus on learning by a tutor
                   presenting objects and not by providing a dataset.
                   Recognition of spatial relations works well, however
                   no definitive conclusions can be drawn. This is
                   largely due to the small number of participants and
                   the subjective nature of spatial relations.},
  url =           {http://hdl.handle.net/2077/66207},
}

@inproceedings{Greco:2019aa,
  address =       {Florence, Italy},
  author =        {Greco, Claudio and Plank, Barbara and
                   Fern{\'a}ndez, Raquel and Bernardi, Raffaella},
  booktitle =     {Proceedings of the 57th Annual Meeting of the
                   Association for Computational Linguistics},
  month =         jul,
  pages =         {3601--3605},
  publisher =     {Association for Computational Linguistics},
  title =         {Psycholinguistics Meets Continual Learning: Measuring
                   Catastrophic Forgetting in Visual Question Answering},
  year =          {2019},
  abstract =      {We study the issue of catastrophic forgetting in the
                   context of neural multimodal approaches to Visual
                   Question Answering (VQA). Motivated by evidence from
                   psycholinguistics, we devise a set of
                   linguistically-informed VQA tasks, which differ by
                   the types of questions involved (Wh-questions and
                   polar questions). We test what impact task difficulty
                   has on continual learning, and whether the order in
                   which a child acquires question types facilitates
                   computational models. Our results show that dramatic
                   forgetting is at play and that task difficulty and
                   order matter. Two well-known current continual
                   learning methods mitigate the problem only to a
                   limiting degree.},
  doi =           {10.18653/v1/P19-1350},
  url =           {https://www.aclweb.org/anthology/P19-1350},
}

@article{Hamilton:2014aa,
  author =        {Hamilton, Antonia F de C and Kessler, Klaus and
                   Creem-Regehr, Sarah H},
  journal =       {Frontiers in Human Neuroscience},
  month =         {June},
  number =        {403},
  pages =         {1--3},
  publisher =     {Frontiers Media S.A.},
  title =         {Perspective taking: building a neurocognitive
                   framework for integrating the ``social''and the
                   ``spatial''},
  volume =        {8},
  year =          {2014},
  doi =           {10.3389/fnhum.2014.00403},
  isbn =          {1662-5161},
  url =           {http://www.ncbi.nlm.nih.gov/pmc/articles/PMC4052522/},
}

@article{Harnad:1990,
  author =        {Harnad, Stevan},
  journal =       {Physica D},
  month =         {June},
  number =        {1--3},
  pages =         {335--346},
  title =         {The symbol grounding problem},
  volume =        {42},
  year =          {1990},
  doi =           {10.1016/0167-2789(90)90087-6},
}

@inproceedings{Hudson:2019aa,
  author =        {Hudson, Drew A and Manning, Christopher D},
  booktitle =     {Proceedings of the IEEE Conference on Computer Vision
                   and Pattern Recognition},
  pages =         {6700--6709},
  title =         {Gqa: A new dataset for real-world visual reasoning
                   and compositional question answering},
  year =          {2019},
  url =           {http://openaccess.thecvf.com/content_CVPR_2019/html/
                  Hudson_GQA_A_New_Dataset_for_Real-
  World_Visual_Reasoning_and_Compositional_CVPR_2019_paper.html},
}

@article{Jordan:2005aa,
  author =        {Jordan, Pamela W and Walker, Marilyn A},
  journal =       {Journal of Artificial Intelligence Research},
  pages =         {157--194},
  title =         {Learning content selection rules for generating
                   object descriptions in dialogue},
  volume =        {24},
  year =          {2005},
  doi =           {http://dx.doi.org/10.1613/jair.1591},
}

@inproceedings{Karpathy:2015aa,
  author =        {Karpathy, Andrej and Fei-Fei, Li},
  booktitle =     {Proceedings of the IEEE Conference on Computer Vision
                   and Pattern Recognition},
  pages =         {3128--3137},
  title =         {Deep visual-semantic alignments for generating image
                   descriptions},
  year =          {2015},
}

@techreport{Karpathy:2015ab,
  author =        {Karpathy, Andrej and Fei-Fei, Li},
  institution =   {The Vision Lab, Stanford University},
  title =         {Automated Image Captioning with ConvNets and
                   Recurrent Nets},
  year =          {2015},
  url =           {https://cs.stanford.edu/people/karpathy/sfmltalk.pdf},
}

@article{Kelleher:2005fk,
  author =        {Kelleher, John D. and Costello, Fintan J. and
                   van Genabith, Josef},
  journal =       {Artificial Intelligence},
  pages =         {62--102},
  title =         {Dynamically Structuring Updating and Interrelating
                   Representations of Visual and Linguistic Discourse},
  volume =        {167},
  year =          {2005},
}

@article{Kelleher:2009fk,
  address =       {Cambridge, MA, USA},
  author =        {Kelleher, John D. and Costello, Fintan J.},
  journal =       {Computational Linguistics},
  number =        {2},
  pages =         {271--306},
  publisher =     {MIT Press},
  title =         {Applying computational models of spatial prepositions
                   to visually situated dialog},
  volume =        {35},
  year =          {2009},
  doi =           {10.1162/coli.06-78-prep14},
  issn =          {0891-2017},
}

@article{Kennington:2014aa,
  author =        {Kennington, Casey and Kousidis, Spyros and
                   Schlangen, David},
  journal =       {Proceedings of SIGdial 2014: Short Papers},
  title =         {{InproTKs:} A toolkit for incremental situated
                   processing},
  year =          {2014},
}

@inproceedings{Kennington:2015aa,
  address =       {Beijing, China},
  author =        {Kennington, Casey and Schlangen, David},
  booktitle =     {Proceedings of the 53rd Annual Meeting of the
                   Association for Computational Linguistics and the 7th
                   International Joint Conference on Natural Language
                   Processing (Volume 1: Long Papers)},
  month =         {July},
  pages =         {292--301},
  publisher =     {Association for Computational Linguistics},
  title =         {Simple Learning and Compositional Application of
                   Perceptually Grounded Word Meanings for Incremental
                   Reference Resolution},
  year =          {2015},
  url =           {http://www.aclweb.org/anthology/P15-1029},
}

@article{Kolve:2017aa,
  author =        {Eric Kolve and Roozbeh Mottaghi and Daniel Gordon and
                   Yuke Zhu and Abhinav Gupta and Ali Farhadi},
  journal =       {arXiv},
  title =         {{AI2-THOR:} An Interactive 3D Environment for Visual
                   {AI}},
  volume =        {arXiv:1712.05474 [cs.CV]},
  year =          {2017},
  url =           {http://arxiv.org/abs/1712.05474},
}

@inproceedings{Krafka:2016aa,
  author =        {Kyle Krafka and Aditya Khosla and Petr Kellnhofer and
                   Harini Kannan and Suchendra Bhandarkar and
                   Wojciech Matusik and Antonio Torralba},
  booktitle =     {IEEE Conference on Computer Vision and Pattern
                   Recognition (CVPR)},
  title =         {Eye Tracking for Everyone},
  year =          {2016},
  url =           {https://people.csail.mit.edu/khosla/papers/
                  cvpr2016_Khosla.pdf},
}

@article{Krahmer:2011aa,
  author =        {Krahmer, Emiel and van Deemter, Kees},
  journal =       {Computational Linguistics},
  number =        {1},
  pages =         {173--218},
  publisher =     {MIT Press},
  title =         {Computational Generation of Referring Expressions: A
                   Survey},
  volume =        {38},
  year =          {2011},
  isbn =          {0891-2017},
}

@incollection{Kruijff:2006ab,
  address =       {Berlin, Heidelberg},
  author =        {Kruijff, Geert-Jan M. and Kelleher, John D. and
                   Hawes, Nick},
  booktitle =     {Perception and Interactive Technologies.
                   International Tutorial and Research Workshop, PIT
                   2006 Kloster Irsee, Germany},
  editor =        {Andr{\'e}, Elisabeth and Dybkj{\ae}r, Laila and
                   Minker, Wolfgang and Neumann, Heiko and
                   Weber, Michael},
  pages =         {117--128},
  publisher =     {Springer},
  title =         {Information Fusion for Visual Reference Resolution in
                   Dynamic Situated Dialogue},
  year =          {2006},
}

@article{Kruijff:2007,
  author =        {Kruijff, Geert-Jan M. and Zender, Hendrik and
                   Jensfelt, Patric and Christensen, Henrik I.},
  journal =       {International Journal of Advanced Robotic Systems},
  note =          {Special issue on human and robot interactive
                   communication},
  number =        {1},
  pages =         {125--138},
  title =         {Situated dialogue and spatial organization: what,
                   where... and why?},
  volume =        {4},
  year =          {2007},
  url =           {http://www.cognitivesystems.org/publications/kruijff_etal07-
                  jars.pdf},
}

@inproceedings{Kulkarni:2011aa,
  author =        {G. {Kulkarni} and V. {Premraj} and S. {Dhar} and
                   S. {Li} and Y. {Choi} and A. C. {Berg} and
                   T. L. {Berg}},
  booktitle =     {CVPR 2011},
  month =         {June},
  pages =         {1601--1608},
  title =         {Baby talk: Understanding and generating simple image
                   descriptions},
  year =          {2011},
  abstract =      {We posit that visually descriptive language offers
                   computer vision researchers both information about
                   the world, and information about how people describe
                   the world. The potential benefit from this source is
                   made more significant due to the enormous amount of
                   language data easily available today. We present a
                   system to automatically generate natural language
                   descriptions from images that exploits both
                   statistics gleaned from parsing large quantities of
                   text data and recognition algorithms from computer
                   vision. The system is very effective at producing
                   relevant sentences for images. It also generates
                   descriptions that are notably more true to the
                   specific image content than previous work.},
  doi =           {10.1109/CVPR.2011.5995466},
  issn =          {1063-6919},
  url =           {https://ieeexplore.ieee.org/document/5995466/
                  authors#authors},
}

@mastersthesis{Lang:2011aa,
  address =       {Berlin, Germany},
  author =        {Simon Lang},
  school =        {Institut f{\"u}r Informatik, Freie Universit{\"a}t
                   Berlin},
  type =          {Bachelorarbeit},
  title =         {Sign Language Recognition with Kinect},
  year =          {2011},
}

@inbook{Lang:2012aa,
  address =       {Berlin, Heidelberg},
  author =        {Lang, Simon and Block, Marco and Rojas, Ra{\'u}l},
  booktitle =     {Artificial Intelligence and Soft Computing: 11th
                   International Conference, ICAISC 2012, Zakopane,
                   Poland, April 29-May 3, 2012, Proceedings, Part I},
  editor =        {Rutkowski, Leszek and Korytkowski, Marcin and
                   Scherer, Rafa{\l} and Tadeusiewicz, Ryszard and
                   Zadeh, Lotfi A. and Zurada, Jacek M.},
  pages =         {394--402},
  publisher =     {Springer Berlin Heidelberg},
  title =         {Sign Language Recognition Using Kinect},
  year =          {2012},
  abstract =      {An open source framework for general gesture
                   recognition is presented and tested with isolated
                   signs of sign language. Other than common systems for
                   sign language recognition, this framework makes use
                   of Kinect, a depth camera which makes real-time
                   3D-reconstruction easily applicable. Recognition is
                   done using hidden Markov models with a continuous
                   observation density. The framework also offers an
                   easy way of initializing and training new gestures or
                   signs by performing them several times in front of
                   the camera. First results with a recognition rate of
                   97{\%} show that depth cameras are well-suited for
                   sign language recognition.},
  doi =           {10.1007/978-3-642-29347-4_46},
  isbn =          {978-3-642-29347-4},
}

@article{Lauria:2001,
  author =        {Lauria, Stanislao and Bugmann, Guido and
                   Kyriacou, Theocharis and Bos, Johan and Klein, Ewan},
  journal =       {IEEE Intelligent Systems},
  month =         {September/October},
  pages =         {38--45},
  title =         {Training personal robots using natural language
                   instruction},
  volume =        {16},
  year =          {2001},
}

@article{LauriaEtAl:2002b,
  author =        {Lauria, Stanislao and Bugmann, Guido and
                   Kyriacou, Theocharis and Klein, Ewan},
  journal =       {Robotics and Autonomous Systems},
  number =        {3--4},
  pages =         {171--181},
  title =         {Mobile robot programming using natural language},
  volume =        {38},
  year =          {2002},
  abstract =      {How will naive users program domestic robots? This
                   paper describes the design of a practical system that
                   uses natural language to teach a vision-based robot
                   how to navigate in a miniature town. To enable
                   unconstrained speech the robot is provided with a set
                   of primitive procedures derived from a corpus of
                   route instructions. When the user refers to a route
                   that is not known to the robot, the system will learn
                   it by combining primitives as instructed by the user.
                   This paper describes the components of the
                   Instruction-Based Learning architecture and discusses
                   issues of knowledge representation, the selection of
                   primitives and the conversion of natural language
                   into robot-understandable procedures.},
  doi =           {http://dx.doi.org/10.1016/S0921-8890(02)00166-5},
}

@article{Lazaridou:2016aa,
  author =        {Angeliki Lazaridou and Alexander Peysakhovich and
                   Marco Baroni},
  journal =       {arXiv},
  pages =         {1--11},
  title =         {Multi-Agent Cooperation and the Emergence of
                   (Natural) Language},
  volume =        {arXiv:1612.07182v2 [cs.CL]},
  year =          {2016},
  url =           {http://arxiv.org/abs/1612.07182},
}

@incollection{LoganSadler:1996,
  address =       {Cambridge, MA},
  author =        {Logan, Gordon D. and Sadler, Daniel D.},
  booktitle =     {Language and Space},
  editor =        {Bloom, Paul and Peterson, Mary A. and Nadel, Lynn and
                   Garrett, Merrill F.},
  pages =         {493--530},
  publisher =     {MIT Press},
  title =         {A computational analysis of the apprehension of
                   spatial relations},
  year =          {1996},
}

@inproceedings{Lowe:1999aa,
  author =        {Lowe, David G},
  booktitle =     {Computer vision, 1999. The proceedings of the seventh
                   IEEE international conference on},
  organization =  {IEEE},
  pages =         {1150--1157},
  title =         {Object recognition from local scale-invariant
                   features},
  volume =        {2},
  year =          {1999},
  doi =           {10.1109/ICCV.1999.790410},
}

@article{Lowe:2004aa,
  author =        {Lowe, David G},
  journal =       {International journal of computer vision},
  number =        {2},
  pages =         {91--110},
  publisher =     {Springer},
  title =         {Distinctive image features from scale-invariant
                   keypoints},
  volume =        {60},
  year =          {2004},
}

@unpublished{Lu:2016aa,
  author =        {Jiasen Lu and Caiming Xiong and Devi Parikh and
                   Richard Socher},
  month =         {6 June},
  note =          {arXiv:1612.01887 [cs.CV]},
  title =         {Knowing When to Look: Adaptive Attention via A Visual
                   Sentinel for Image Captioning},
  year =          {2017},
}

@inproceedings{Malinowski:2015aa,
  author =        {Malinowski, Mateusz and Rohrbach, Marcus and
                   Fritz, Mario},
  booktitle =     {Proceedings of the IEEE International Conference on
                   Computer Vision},
  pages =         {1--9},
  title =         {Ask your neurons: A neural-based approach to
                   answering questions about images},
  year =          {2015},
}

@phdthesis{Malinowski:2017aa,
  address =       {Saarbr{\"u}cken, Germany},
  author =        {Malinowski, Mateusz},
  month =         {June},
  school =        {Doctor of Engineering Science},
  type =          {Doctor of Engineering Science},
  title =         {Towards holistic machines : From visual recognition
                   to question answering about real-world images},
  year =          {2017},
  abstract =      {Computer Vision has undergone major changes over the
                   recent five years. Here, we investigate if the
                   performance of such architectures generalizes to more
                   complex tasks that require a more holistic approach
                   to scene comprehension. The presented work focuses on
                   learning spatial and multi-modal representations, and
                   the foundations of a Visual Turing Test, where the
                   scene understanding is tested by a series of
                   questions about its content. In our studies, we
                   propose DAQUAR, the first `question answering about
                   real-world images' dataset together with methods,
                   termed a symbolic-based and a neural-based visual
                   question answering architectures, that address the
                   problem. The symbolic-based method relies on a
                   semantic parser, a database of visual facts, and a
                   bayesian formulation that accounts for various
                   interpretations of the visual scene. The neural-based
                   method is an end-to-end architecture composed of a
                   question encoder, image encoder, multimodal
                   embedding, and answer decoder. This architecture has
                   proven to be effective in capturing language-based
                   biases. It also becomes the standard component of
                   other visual question answering architectures. Along
                   with the methods, we also investigate various
                   evaluation metrics that embraces uncertainty in
                   word's meaning, and various interpretations of the
                   scene and the question.},
  doi =           {doi:10.22028/D291-26773},
}

@article{Marge:2019aa,
  address =       {New York, NY, USA},
  author =        {Marge, Matthew and Rudnicky, Alexander I.},
  journal =       {ACM Trans. Interact. Intell. Syst.},
  month =         feb,
  number =        {1},
  pages =         {3:1--3:40},
  publisher =     {ACM},
  title =         {Miscommunication Detection and Recovery in Situated
                   Human\&\#x02013;Robot Dialogue},
  volume =        {9},
  year =          {2019},
  doi =           {10.1145/3237189},
  issn =          {2160-6455},
  url =           {http://doi.acm.org/10.1145/3237189},
}

@techreport{Marge:2019ab,
  address =       {Gothenburg, Sweden},
  author =        {Marge, Matthew},
  institution =   {Army Research Lab},
  month =         {September 9},
  type =          {Presentation in the CLASP seminar},
  title =         {Towards Natural Dialogue with Robots},
  year =          {2019},
  url =           {https://drive.google.com/file/d/1P3Zk88WVG37j9jDPxlV8zUGCea-
                  eDE6U/view},
}

@book{Marr:1982aa,
  author =        {Marr, David},
  month =         {August},
  publisher =     {MIT Press Scholarship Online},
  title =         {Vision: A computational approach},
  year =          {2010},
  annote =        {This posthumously published book (1982), which
                   influenced a generation of brain and cognitive
                   scientists, inspiring many to enter the field,
                   describes a general framework for understanding
                   visual perception and touches on broader questions
                   about how the brain and its functions can be studied
                   and understood. This MIT Press edition makes this
                   work available to a new generation of students and
                   scientists. In the author's framework, the process of
                   vision constructs a set of representations, starting
                   from a description of the input image and culminating
                   with a description of three-dimensional objects in
                   the surrounding environment. A central theme, and one
                   that has had far-reaching influence in both
                   neuroscience and cognitive science, is the notion of
                   different levels of analysis---in the author's
                   framework, the computational level, the algorithmic
                   level, and the hardware implementation level. Now,
                   thirty years later, the main problems that occupied
                   the author remain fundamental open problems in the
                   study of perception. His book provides inspiration
                   for the continuing efforts to integrate knowledge
                   from cognition and computation to understand vision
                   and the brain.},
  doi =           {10.7551/mitpress/9780262514620.001.0001},
}

@mastersthesis{Matsson:2018aa,
  address =       {Gothenburg, Sweden},
  author =        {Matsson, Arild},
  month =         {September 24},
  note =          {Examiner: Peter Ljungl{\"o}f; supervisors: Simon
                   Dobnik and Staffan Larsson; opponent: Axel Almqvist},
  school =        {Masters in Language Technology (MLT), Department of
                   Philosophy, Linguistics and Theory of Science.
                   University of Gothenburg},
  type =          {Masters in Language Technology (MLT), 30 HEC},
  title =         {Implementing perceptual semantics in Type Theory with
                   Records (TTR)},
  year =          {2018},
  url =           {http://hdl.handle.net/2077/62521},
}

@inproceedings{Matuszek:2012aa,
  address =       {Edinburgh, Scotland},
  author =        {Matuszek, Cynthia and FitzGerald, Nicholas and
                   Zettlemoyer, Luke and Bo, Liefeng and Fox, Dieter},
  booktitle =     {Proceedings of the 29th International Conference on
                   Machine Learning (ICML 2012)},
  editor =        {John Langford and Joelle Pineau},
  month =         {June 27th - July 3rd},
  title =         {A joint model of language and perception for grounded
                   attribute learning},
  year =          {2012},
}

@inproceedings{Matuszek:2012uq,
  author =        {Cynthia Matuszek and Evan Herbst and Luke Zettlemoyer and
                   Dieter Fox},
  booktitle =     {Proceedings of the 13th International Symposium on
                   Experimental Robotics (ISER)},
  month =         {June},
  title =         {Learning to Parse Natural Language Commands to a
                   Robot Control System},
  year =          {2012},
}

@article{McMahan:2015aa,
  author =        {McMahan, Brian and Stone, Matthew},
  journal =       {Transactions of the Association for Computational
                   Linguistics},
  pages =         {103--115},
  title =         {A {B}ayesian model of grounded color semantics},
  volume =        {3},
  year =          {2015},
}

@inproceedings{Mei:2016aa,
  author =        {Mei, Hongyuan and Bansal, Mohit and
                   Walter, Matthew R},
  booktitle =     {AAAI},
  pages =         {2772--2778},
  title =         {Listen, Attend, and Walk: Neural Mapping of
                   Navigational Instructions to Action Sequences},
  year =          {2016},
}

@article{Monroe:2016aa,
  author =        {Will Monroe and Noah D. Goodman and
                   Christopher Potts},
  journal =       {CoRR},
  title =         {Learning to Generate Compositional Color
                   Descriptions},
  volume =        {abs/1606.03821},
  year =          {2016},
  url =           {http://arxiv.org/abs/1606.03821},
}

@article{Muja:2009aa,
  author =        {Muja, Marius and Lowe, David G},
  journal =       {VISAPP (1)},
  number =        {331--340},
  pages =         {2},
  title =         {Fast approximate nearest neighbors with automatic
                   algorithm configuration},
  volume =        {2},
  year =          {2009},
}

@incollection{Mukerjee:1998,
  address =       {Mahwah, N.J.},
  author =        {Mukerjee, Amitabha},
  booktitle =     {Representation and Processing of Spatial Expressions},
  editor =        {Olivier, Patrick and Gapp, Klaus-Peter},
  pages =         {1--36},
  publisher =     {Lawrence Erlbaum Associates},
  title =         {Neat versus scruffy: a review of computational models
                   of spatial expressions},
  year =          {1998},
}

@inproceedings{Mustafa:2014aa,
  author =        {Mustafa, Edon and Dimopoulos, Konstantinos},
  booktitle =     {Dautov, R., Gkasis, P., \& Karama-nos, A. et
                   al.(2014). Proceedings of the 9th South East
                   EuropeanDoctoral Student Conference},
  organization =  {Thessaloniki: SEERC},
  pages =         {271--285},
  title =         {Sign Language Recognition using Kinect},
  year =          {2014},
}

@book{OKane:2013aa,
  author =        {Jason M. O'Kane},
  month =         {10},
  publisher =     {CreateSpace Independent Publishing Platform},
  title =         {A Gentle Introduction to ROS},
  year =          {2013},
  isbn =          {9781492143239},
}

@book{Oviatt:2017aa,
  address =       {New York, NY, USA},
  author =        {Oviatt, Sharon and Schuller, Bj\"{o}rn and
                   Cohen, Philip R. and Sonntag, Daniel and
                   Potamianos, Gerasimos and Kr\"{u}ger, Antonio},
  publisher =     {Association for Computing Machinery and Morgan \&
                   Claypool},
  title =         {The Handbook of Multimodal-Multisensor Interfaces:
                   Foundations, User Modeling, and Common Modality
                   Combinations - Volume 1},
  volume =        {1},
  year =          {2017},
  isbn =          {978-1-97000-167-9},
}

@article{Pezzelle:2018aa,
  author =        {Sandro Pezzelle and Ionut{-}Teodor Sorodoc and
                   Raffaella Bernardi},
  journal =       {arXiv},
  pages =         {1--12},
  title =         {Comparatives, Quantifiers, Proportions: {A}
                   Multi-Task Model for the Learning of Quantities from
                   Vision},
  volume =        {arXiv:1804.05018 [cs.CV]},
  year =          {2018},
  url =           {http://arxiv.org/abs/1804.05018},
}

@article{Pezzulo:2011aa,
  author =        {Pezzulo, Giovanni and Barsalou, Lawrence and
                   Cangelosi, Angelo and Fischer, Martin and
                   Spivey, Michael and McRae, Ken},
  journal =       {Frontiers in Psychology},
  pages =         {5},
  title =         {The Mechanics of Embodiment: A Dialog on Embodiment
                   and Computational Modeling},
  volume =        {2},
  year =          {2011},
  abstract =      {Embodied theories are increasingly challenging
                   traditional views of cognition by arguing that
                   conceptual representations that constitute our
                   knowledge are grounded in sensory and motor
                   experiences, and processed at this sensorimotor
                   level, rather than being represented and processed
                   abstractly in an amodal conceptual system. Given the
                   established empirical foundation, and the relatively
                   underspecified theories to date, many researchers are
                   extremely interested in embodied cognition but are
                   clamouring for more mechanistic implementations. What
                   is needed at this stage is a push toward explicit
                   computational models that implement sensory-motor
                   grounding as intrinsic to cognitive processes. In
                   this article, six authors from varying backgrounds
                   and approaches address issues concerning the
                   construction of embodied computational models, and
                   illustrate what they view as the critical current and
                   next steps toward mechanistic theories of embodiment.
                   The first part has the form of a dialogue between two
                   fictional characters: Ernest, the ``experimenter'',
                   and Mary, the ``computational modeller''. The
                   dialogue consists of an interactive sequence of
                   questions, requests for clarification, challenges,
                   and (tentative) answers, and touches the most
                   important aspects of grounded theories that should
                   inform computational modeling and, conversely, the
                   impact that computational modeling could have on
                   embodied theories. The second part of the article
                   discusses the most important open challenges for
                   embodied computational modelling.},
  doi =           {10.3389/fpsyg.2011.00005},
  issn =          {1664-1078},
  url =           {https://www.frontiersin.org/article/10.3389/
                  fpsyg.2011.00005},
}

@techreport{Pustejovsky:2020aa,
  author =        {Pustejovsky, James and Krishnaswamy, Nikhil},
  institution =   {Department of Computer Science, Brandeis University},
  month =         {July},
  type =          {Journal article manuscript},
  title =         {Situated Meaning in Multimodal Dialogue: Human-Robot
                   and Human-Computer Interactions},
  year =          {2020},
  url =           {http://www.voxicon.net/wp-content/uploads/2020/07/TAL_2020-
                  13.pdf},
}

@inproceedings{Quigley:2009aa,
  author =        {Quigley, Morgan and Conley, Ken and Gerkey, Brian and
                   Faust, Josh and Foote, Tully and Leibs, Jeremy and
                   Wheeler, Rob and Ng, Andrew Y},
  booktitle =     {ICRA workshop on open source software},
  pages =         {5},
  title =         {{ROS:} an open-source Robot Operating System},
  volume =        {3},
  year =          {2009},
  url =           {http://www.ros.org/},
}

@inproceedings{Ramisa:2015aa,
  address =       {Lisbon, Portugal},
  author =        {Ramisa, Arnau and Wang, Josiah and Lu, Ying and
                   Dellandrea, Emmanuel and Moreno-Noguer, Francesc and
                   Gaizauskas, Robert},
  booktitle =     {Proceedings of the 2015 Conference on Empirical
                   Methods in Natural Language Processing},
  month =         {7--21 September},
  organization =  {Association for Computational Linguistics},
  pages =         {214--220},
  title =         {Combining geometric, textual and visual features for
                   predicting prepositions in image descriptions},
  year =          {2015},
  url =           {http://www.aclweb.org/anthology/D/D15/D15-1022.pdf},
}

@inproceedings{Rashtchian:2010kx,
  address =       {Los Angeles, CA},
  author =        {Cyrus Rashtchian and Peter Young and Micah Hodosh and
                   Julia Hockenmaier},
  booktitle =     {Proceedings of the {NAACL HLT} 2010 {W}orkshop on
                   creating speech and language data with {Amazon's
                   Mechanical Turk}},
  month =         {6 June},
  publisher =     {North American Chapter of the Association for
                   Computational Linguistics (NAACL)},
  title =         {Collecting Image Annotations Using {Amazon's
                   Mechanical Turk}},
  year =          {2010},
}

@book{Regier:1996,
  address =       {Cambridge, Massachusetts, London, England},
  author =        {Regier, Terry},
  publisher =     {MIT Press},
  title =         {The human semantic potential: spatial language and
                   constrained connectionism},
  year =          {1996},
}

@article{RegierCarlson:2001,
  author =        {Regier, Terry and Carlson, Laura A.},
  journal =       {Journal of Experimental Psychology: General},
  number =        {2},
  pages =         {273--298},
  title =         {Grounding spatial language in perception: an
                   empirical and computational investigation},
  volume =        {130},
  year =          {2001},
  doi =           {10.1037//0096-3445.130.2.273},
}

@inproceedings{Rennie:2017aa,
  author =        {Rennie, Steven J and Marcheret, Etienne and
                   Mroueh, Youssef and Ross, Jerret and Goel, Vaibhava},
  booktitle =     {Proceedings of the IEEE Conference on Computer Vision
                   and Pattern Recognition},
  pages =         {7008--7024},
  title =         {Self-critical sequence training for image captioning},
  year =          {2017},
}

@article{Retz-Schmidt:1988aa,
  author =        {Retz-Schmidt, Gudula},
  journal =       {AI magazine},
  number =        {2},
  pages =         {95--95},
  title =         {Various views on spatial prepositions},
  volume =        {9},
  year =          {1988},
  url =           {https://wvvw.aaai.org/ojs/index.php/aimagazine/article/
                  download/678/596},
}

@phdthesis{Rosman:2014aa,
  address =       {Edinburgh},
  author =        {Rosman, Benjamin Saul},
  school =        {Institute of Perception, Action and Behaviour, School
                   of Informatics, The University of Edinburgh},
  type =          {Doctor of Philosophy},
  title =         {Learning domain abstractions for long lived robots},
  year =          {2014},
  abstract =      {Recent trends in robotics have seen more general
                   purpose robots being deployed in unstructured
                   environments for prolonged periods of time. Such
                   robots are expected to adapt to different
                   environmental conditions, and ultimately take on a
                   broader range of responsibilities, the specifications
                   of which may change online after the robot has been
                   deployed. We propose that in order for a robot to be
                   generally capable in an online sense when it
                   encounters a range of unknown tasks, it must have the
                   ability to continually learn from a lifetime of
                   experience. Key to this is the ability to generalise
                   from experiences and form representations which
                   facilitate faster learning of new tasks, as well as
                   the transfer of knowledge between different
                   situations. However, experience cannot be managed
                   navely: one does not want constantly expanding
                   tables of data, but instead continually refined
                   abstractions of the data -- much like humans seem to
                   abstract and organise knowledge. If this agent is
                   active in the same, or similar, classes of
                   environments for a prolonged period of time, it is
                   provided with the opportunity to build abstract
                   representations in order to simplify the learning of
                   future tasks. The domain is a common structure
                   underlying large families of tasks, and exploiting
                   this affords the agent the potential to not only
                   minimise relearning from scratch, but over time to
                   build better models of the environment. We propose to
                   learn such regularities from the environment, and
                   extract the commonalities between tasks. This thesis
                   aims to address the major question: what are the
                   domain invariances which should be learnt by a long
                   lived agent which encounters a range of different
                   tasks? This question can be decomposed into three
                   dimensions for learning invariances, based on
                   perception, action and interaction. We present novel
                   algorithms for dealing with each of these three
                   factors. Firstly, how does the agent learn to
                   represent the structure of the world? We focus here
                   on learning inter-object relationships from depth
                   information as a concise representation of the
                   structure of the domain. To this end we introduce
                   contact point networks as a topological abstraction
                   of a scene, and present an algorithm based on support
                   vector machine decision boundaries for extracting
                   these from three dimensional point clouds obtained
                   from the agent's experience of a domain. By reducing
                   the specific geometry of an environment into general
                   skeletons based on contact between different objects,
                   we can autonomously learn predicates describing
                   spatial relationships. Secondly, how does the agent
                   learn to acquire general domain knowledge? While the
                   agent attempts new tasks, it requires a mechanism to
                   control exploration, particularly when it has many
                   courses of action available to it. To this end we
                   draw on the fact that many local behaviours are
                   common to different tasks. Identifying these amounts
                   to learning ``common sense'' behavioural invariances
                   across multiple tasks. This principle leads to our
                   concept of action priors, which are defined as
                   Dirichlet distributions over the action set of the
                   agent. These are learnt from previous behaviours, and
                   expressed as the prior probability of selecting each
                   action in a state, and are used to guide the learning
                   of novel tasks as an exploration policy within a
                   reinforcement learning framework. Finally, how can
                   the agent react online with sparse information? There
                   are times when an agent is required to respond fast
                   to some interactive setting, when it may have
                   encountered similar tasks previously. To address this
                   problem, we introduce the notion of types, being a
                   latent class variable describing related problem
                   instances. The agent is required to learn, identify
                   and respond to these different types in online
                   interactive scenarios. We then introduce Bayesian
                   policy reuse as an algorithm that involves
                   maintaining beliefs over the current task instance,
                   updating these from sparse signals, and selecting and
                   instantiating an optimal response from a behaviour
                   library. This thesis therefore makes the following
                   contributions. We provide the first algorithm for
                   autonomously learning spatial relationships between
                   objects from point cloud data. We then provide an
                   algorithm for extracting action priors from a set of
                   policies, and show that considerable gains in speed
                   can be achieved in learning subsequent tasks over
                   learning from scratch, particularly in reducing the
                   initial losses associated with unguided exploration.
                   Additionally, we demonstrate how these action priors
                   allow for safe exploration, feature selection, and a
                   method for analysing and advising other agents'
                   movement through a domain. Finally, we introduce
                   Bayesian policy reuse which allows an agent to
                   quickly draw on a library of policies and instantiate
                   the correct one, enabling rapid online responses to
                   adversarial conditions.},
  url =           {http://hdl.handle.net/1842/9665},
}

@article{Roy:2002,
  author =        {Roy, Deb},
  journal =       {Computer speech and language},
  number =        {3},
  pages =         {353--385},
  title =         {Learning visually-grounded words and syntax for a
                   scene description task},
  volume =        {16},
  year =          {2002},
  abstract =      {A spoken language generation system has been
                   developed that learns to describe objects in
                   computer-generated visual scenes. The system is
                   trained by a show-and-tell' procedure in which visual
                   scenes are paired with natural language descriptions.
                   Learning algorithms acquire probabilistic structures
                   which encode the visual semantics of phrase
                   structure, word classes, and individual words. Using
                   these structures, a planning algorithm integrates
                   syntactic, semantic, and contextual constraints to
                   generate natural and unambiguous descriptions of
                   objects in novel scenes. The system generates
                   syntactically well-formed compound adjective noun
                   phrases, as well as relative spatial clauses. The
                   acquired linguistic structures generalize from
                   training data, enabling the production of novel word
                   sequences which were never observed during training.
                   The output of the generation system is synthesized
                   using word-based concatenative synthesis drawing from
                   the original training speech corpus. In evaluations
                   of semantic comprehension by human judges, the
                   performance of automatically generated spoken
                   descriptions was comparable to human-generated
                   descriptions. This work is motivated by our long-term
                   goal of developing spoken language processing systems
                   which grounds semantics in machine perception and
                   action.},
}

@article{Roy:2005,
  address =       {Essex, UK},
  author =        {Roy, Deb},
  journal =       {Artificial Intelligence},
  month =         {September},
  number =        {1-2},
  pages =         {170--205},
  publisher =     {Elsevier Science Publishers Ltd.},
  title =         {Semiotic schemas: a framework for grounding language
                   in action and perception},
  volume =        {167},
  year =          {2005},
  doi =           {10.1016/j.artint.2005.04.007},
  issn =          {0004-3702},
}

@book{Russell:2016aa,
  author =        {Russell, Stuart J and Norvig, Peter and
                   Davis, Ernest},
  publisher =     {Pearson Education M.U.A.},
  series =        {Prentice Hall series in Artificial Intelligence},
  title =         {Artificial Intelligence: A Modern Approach},
  year =          {2016},
  isbn =          {1292153962},
}

@article{Savva:2019aa,
  author =        {Manolis Savva and Abhishek Kadian and
                   Oleksandr Maksymets and Yili Zhao and Erik Wijmans and
                   Bhavana Jain and Julian Straub and Jia Liu and
                   Vladlen Koltun and Jitendra Malik and Devi Parikh and
                   Dhruv Batra},
  journal =       {arXiv},
  pages =         {1--16},
  title =         {Habitat: A Platform for Embodied AI Research},
  volume =        {arXiv:1904.01201 [cs.CV]},
  year =          {2019},
  abstract =      {We present Habitat, a platform for research in
                   embodied artificial intelligence (AI). Habitat
                   enables training embodied agents (virtual robots) in
                   highly efficient photorealistic 3D simulation.
                   Specifically, Habitat consists of: (i) Habitat-Sim: a
                   flexible, high-performance 3D simulator with
                   configurable agents, sensors, and generic 3D dataset
                   handling. Habitat-Sim is fast -- when rendering a
                   scene from Matterport3D, it achieves several thousand
                   frames per second (fps) running single-threaded, and
                   can reach over 10,000 fps multi-process on a single
                   GPU. (ii) Habitat-API: a modular high-level library
                   for end-to-end development of embodied AI algorithms
                   -- defining tasks (e.g. navigation, instruction
                   following, question answering), configuring,
                   training, and benchmarking embodied agents. These
                   large-scale engineering contributions enable us to
                   answer scientific questions requiring experiments
                   that were till now impracticable or `merely'
                   impractical. Specifically, in the context of
                   point-goal navigation: (1) we revisit the comparison
                   between learning and SLAM approaches from two recent
                   works and find evidence for the opposite conclusion
                   -- that learning outperforms SLAM if scaled to an
                   order of magnitude more experience than previous
                   investigations, and (2) we conduct the first
                   cross-dataset generalization experiments {train,
                   test}  {Matterport3D, Gibson} for multiple sensors
                   {blind, RGB, RGBD, D} and find that only agents with
                   depth (D) sensors generalize across datasets. We hope
                   that our open-source platform and these findings will
                   advance research in embodied AI.},
  url =           {https://arxiv.org/abs/1904.01201},
}

@article{Scheutz:2011aa,
  author =        {Scheutz, Matthias and Cantrell, Rehj and
                   Schermerhorn, Paul},
  journal =       {AI Magazine},
  month =         {December},
  number =        {4},
  pages =         {77--84},
  title =         {Toward Humanlike Task-Based Dialogue Processing for
                   Human Robot Interaction},
  volume =        {32},
  year =          {2011},
  doi =           {10.1609/aimag.v32i4.2381},
  url =           {https://aaai.org/ojs/index.php/aimagazine/article/view/
                  2381},
}

@article{Shekhar:2019aa,
  author =        {Ravi Shekhar and Ece Takmaz and Raquel Fern{\'a}ndez and
                   Raffaella Bernardi},
  journal =       {arXiv},
  title =         {Evaluating the Representational Hub of Language and
                   Vision Models},
  volume =        {arXiv:1904.06038 [cs.CL]},
  year =          {2019},
  url =           {https://arxiv.org/abs/1904.06038},
}

@inproceedings{Shutova:2016aa,
  address =       {San Diego, California},
  author =        {Shutova, Ekaterina and Kiela, Douwe and
                   Maillard, Jean},
  booktitle =     {Proceedings of the 2016 Conference of the North
                   American Chapter of the Association for Computational
                   Linguistics: Human Language Technologies},
  month =         {June},
  pages =         {160--170},
  publisher =     {Association for Computational Linguistics},
  title =         {Black Holes and White Rabbits: Metaphor
                   Identification with Visual Features},
  year =          {2016},
  url =           {http://www.aclweb.org/anthology/N16-1020},
}

@inproceedings{Skantze:2012aa,
  author =        {Skantze, Gabriel and Al Moubayed, Samer},
  booktitle =     {Proceedings of the 14th ACM international conference
                   on Multimodal interaction},
  organization =  {ACM},
  pages =         {69--76},
  title =         {{IrisTK}: a statechart-based toolkit for multi-party
                   face-to-face interaction},
  year =          {2012},
}

@article{Skantze:2014aa,
  author =        {Skantze, Gabriel and Hjalmarsson, Anna and
                   Oertel, Catharine},
  journal =       {Speech Communication},
  pages =         {50--66},
  publisher =     {Elsevier},
  title =         {Turn-taking, feedback and joint attention in situated
                   human-robot interaction},
  volume =        {65},
  year =          {2014},
  abstract =      {In this paper, we present a study where a robot
                   instructs a human on how to draw a route on a map.
                   The human and robot are seated face-to-face with the
                   map placed on the table between them. The user's and
                   the robot's gaze can thus serve several simultaneous
                   functions: as cues to joint attention, turn-taking,
                   level of understanding and task progression. We have
                   compared this face-to-face setting with a setting
                   where the robot employs a random gaze behaviour, as
                   well as a voice-only setting where the robot is
                   hidden behind a paper board. In addition to this, we
                   have also manipulated turn-taking cues such as
                   completeness and filled pauses in the robot's speech.
                   By analysing the participants' subjective rating,
                   task completion, verbal responses, gaze behaviour,
                   and drawing activity, we show that the users indeed
                   benefit from the robot's gaze when talking about
                   landmarks, and that the robot's verbal and gaze
                   behaviour has a strong effect on the users'
                   turn-taking behaviour. We also present an analysis of
                   the users' gaze and lexical and prosodic realisation
                   of feedback after the robot instructions, and show
                   that these cues reveal whether the user has yet
                   executed the previous instruction, as well as the
                   user's level of uncertainty.},
}

@article{Skantze:2016aa,
  author =        {Skantze, Gabriel},
  journal =       {AI Magazine},
  number =        {4},
  pages =         {19--31},
  title =         {Real-time Coordination in Human-robot Interaction
                   using Face and Voice},
  volume =        {37},
  year =          {2016},
}

@inproceedings{Skocaj:2010fk,
  address =       {Anchorage, AK, USA},
  author =        {Danijel Sko\v{c}aj and Miroslav Jani\v{c}ek and
                   Matej Kristan and Geert-Jan M. Kruijff and
                   Ale\v{s} Leonardis and Pierre Lison and
                   Alen Vre\v{c}ko and Michael Zillich},
  booktitle =     {ICRA 2010 workshop ICAIR - Interactive Communication
                   for Autonomous Intelligent Robots},
  pages =         {30--36},
  title =         {A basic cognitive system for interactive continuous
                   learning of visual concepts},
  year =          {2010},
  abstract =      {Interactive continuous learning is an important
                   characteristic of a cognitive agent that is supposed
                   to operate and evolve in an everchanging environment.
                   In this paper we present representations and
                   mechanisms that are necessary for continuous learning
                   of visual concepts in dialogue with a tutor. We
                   present an approach for modelling beliefs stemming
                   from multiple modalities and we show how these
                   beliefs are created by processing visual and
                   linguistic information and how they are used for
                   learning. We also present a system that exploits
                   these representations and mechanisms, and demonstrate
                   these principles in the case of learning about object
                   colours and basic shapes in dialogue with the tutor.},
}

@inproceedings{Skocaj:2011fu,
  address =       {San Francisco, CA, USA},
  author =        {Danijel Sko\v{c}aj and Matej Kristan and
                   Alen Vre\v{c}ko and Marko Mahni\v{c} and
                   Miroslav Jan\'{i}\v{c}ek and Geert-Jan M. Kruijff and
                   Marc Hanheide and Nick Hawes and Thomas Keller and
                   Michael Zillich and Kai Zhou},
  booktitle =     {IEEE/RSJ International Conference on Intelligent
                   Robots and Systems IROS 2011},
  month =         {25-30 September},
  title =         {A system for interactive learning in dialogue with a
                   tutor},
  year =          {2011},
  abstract =      {In this paper we present representations and
                   mechanisms that facilitate continuous learning of
                   visual concepts in dialogue with a tutor and show the
                   implemented robot system. We present how beliefs
                   about the world are created by processing visual and
                   linguistic information and show how they are used for
                   planning system behaviour with the aim at satisfying
                   its internal drive -- to extend its knowledge. The
                   system facilitates different kinds of learning
                   initiated by the human tutor or by the system itself.
                   We demonstrate these principles in the case of
                   learning about object colours and basic shapes.},
  url =           {http://cogx.eu/data/cogx/publications/skocajIROS11.pdf},
}

@article{Steels:2005os,
  author =        {Steels, Luc and Belpaeme, Tony},
  journal =       {Behavioral and Brain Sciences},
  number =        {4},
  pages =         {469-489},
  title =         {Coordinating Perceptually Grounded Categories Through
                   Language: A Case Study For Colour},
  volume =        {28},
  year =          {2005},
}

@book{Steels:2012aa,
  address =       {Amsterdam and Philadelphia},
  author =        {Steels, Luc},
  publisher =     {John Benjamins Pub. Co},
  series =        {Advances in interaction studies},
  title =         {Experiments in cultural language evolution},
  year =          {2012},
  url =           {https://gu-se-primo.hosted.exlibrisgroup.com/permalink/f/
                  rmbr1s/46GUB_KOHA1914065},
}

@article{SteelsBaillie:2003,
  author =        {Steels, Luc and Baillie, Jean-Christophe},
  journal =       {Robotics and Autonomous Systems},
  month =         {May},
  number =        {2--3},
  pages =         {163--173},
  title =         {Shared grounding of event descriptions by autonomous
                   robots},
  volume =        {43},
  year =          {2003},
  doi =           {doi:10.1016/S0921-8890(02)00357-3},
}

@incollection{SteelsLoetzsch:2009,
  author =        {Luc Steels and Martin Loetzsch},
  booktitle =     {Spatial Language and Dialogue},
  editor =        {Coventry, Kenny R. and Tenbrink, Thora and
                   Bateman, John. A.},
  publisher =     {Oxford University Press},
  title =         {Perspective Alignment in Spatial Language},
  year =          {2009},
}

@inproceedings{Stoia:2006zr,
  address =       {Sydney, Australia},
  author =        {Stoia, Laura and Shockley, Darla Magdalene and
                   Byron, Donna K. and Fosler-Lussier, Eric},
  booktitle =     {Proceedings of the Fourth International Natural
                   Language Generation Conference},
  month =         {July},
  pages =         {81--88},
  publisher =     {Association for Computational Linguistics},
  title =         {Noun Phrase Generation for Situated Dialogs},
  year =          {2006},
}

@article{Tenenbaum:2011ly,
  author =        {Tenenbaum, Joshua B. and Kemp, Charles and
                   Griffiths, Thomas L. and Goodman, Noah D.},
  journal =       {Science},
  number =        {6022},
  pages =         {1279-1285},
  title =         {How to Grow a Mind: Statistics, Structure, and
                   Abstraction},
  volume =        {331},
  year =          {2011},
  abstract =      {In coming to understand the world{\"A}{\^\i}in
                   learning concepts, acquiring language, and grasping
                   causal relations{\"A}{\^\i}our minds make
                   inferences that appear to go far beyond the data
                   available. How do we do it? This review describes
                   recent approaches to reverse-engineering human
                   learning and cognitive development and, in parallel,
                   engineering more humanlike machine learning systems.
                   Computational models that perform probabilistic
                   inference over hierarchies of flexibly structured
                   representations can address some of the deepest
                   questions about the nature and origins of human
                   thought: How does abstract knowledge guide learning
                   and reasoning from sparse data? What forms does our
                   knowledge take, across different domains and tasks?
                   And how is that abstract knowledge itself acquired?},
  doi =           {10.1126/science.1192788},
}

@inproceedings{Thomason:2019aa,
  author =        {Jesse Thomason and Michael Murray and Maya Cakmak and
                   Luke Zettlemoyer},
  booktitle =     {Conference on Robot Learning (CoRL)},
  title =         {Vision-and-Dialog Navigation},
  year =          {2019},
  url =           {https://arxiv.org/abs/1907.04957},
}

@inproceedings{Thomaz:2005aa,
  author =        {Thomaz, Andrea Lockerd and Berlin, Matt and
                   Breazeal, Cynthia},
  booktitle =     {ROMAN 2005. IEEE International Workshop on Robot and
                   Human Interactive Communication, 2005.},
  organization =  {IEEE},
  pages =         {591--598},
  title =         {An embodied computational model of social
                   referencing},
  year =          {2005},
  url =           {http://robotic.media.mit.edu/wp-content/uploads/sites/7/
                  2015/01/Lockerd_etal_RoMan-05.pdf},
}

@inproceedings{Utescher:2019aa,
  address =       {Gothenburg, Sweden},
  author =        {Utescher, Ronja},
  booktitle =     {Proceedings of the 13th International Conference on
                   Computational Semantics - Student Papers},
  month =         {23{--}27 } # may,
  pages =         {9--14},
  publisher =     {Association for Computational Linguistics},
  title =         {Visual {TTR} - Modelling Visual Question Answering in
                   Type Theory with Records},
  year =          {2019},
  abstract =      {In this paper, I will describe a system that was
                   developed for the task of Visual Question Answering.
                   The system uses the rich type universe of Type Theory
                   with Records (TTR) to model both the utterances about
                   the image, the image itself and classifications made
                   related to the two. At its most basic, the decision
                   of whether any given predicate can be assigned to an
                   object in the image is delegated to a CNN.
                   Consequently, images can be judged as evidence for
                   propositions. The end result is a model whose
                   application of perceptual classifiers to a given
                   image is guided by the accompanying utterance.},
  url =           {https://www.aclweb.org/anthology/W19-0602},
}

@unpublished{Vedaldi:2016aa,
  author =        {Vedaldi, Andrea},
  month =         {March},
  note =          {iV\&L summer school on vision and language, Malta.
  http://www.robots.ox.ac.uk/$\sim$vedaldi/assets/teach/vedaldi16deepcv.pdf},
  title =         {Convolutional Networks for Computer Vision
                   Applications},
  year =          {2016},
  url =           {http://www.robots.ox.ac.uk/~vedaldi/assets/teach/
                  vedaldi16deepcv.pdf},
}

@inproceedings{Vedantam:2015aa,
  author =        {Vedantam, Ramakrishna and Lawrence Zitnick, C. and
                   Parikh, Devi},
  booktitle =     {The IEEE Conference on Computer Vision and Pattern
                   Recognition (CVPR)},
  month =         {June},
  pages =         {4566--4575},
  title =         {CIDEr: Consensus-Based Image Description Evaluation},
  year =          {2015},
  abstract =      {Automatically describing an image with a sentence is
                   a long-standing challenge in computer vision and
                   natural language processing. Due to recent progress
                   in object detection, attribute classification, action
                   recognition, etc., there is renewed interest in this
                   area. However, evaluating the quality of descriptions
                   has proven to be challenging. We propose a novel
                   paradigm for evaluating image descriptions that uses
                   human consensus. This paradigm consists of three main
                   parts: a new triplet-based method of collecting human
                   annotations to measure consensus, a new automated
                   metric that captures consensus, and two new datasets:
                   PASCAL-50S and ABSTRACT-50S that contain 50 sentences
                   describing each image. Our simple metric captures
                   human judgment of consensus better than existing
                   metrics across sentences generated by various
                   sources. We also evaluate five state-of-the-art image
                   description approaches using this new protocol and
                   provide a benchmark for future comparisons. A version
                   of CIDEr named CIDEr-D is available as a part of MS
                   COCO evaluation server to enable systematic
                   evaluation and benchmarking.},
  url =           {https://www.cv-foundation.org/openaccess/content_cvpr_2015/
                  html/Vedantam_CIDEr_Consensus-
                  Based_Image_2015_CVPR_paper.html},
}

@inproceedings{Viethen:2008aa,
  author =        {Viethen, Jette and Dale, Robert},
  booktitle =     {Proceedings of the Fifth International Natural
                   Language Generation Conference},
  organization =  {Association for Computational Linguistics},
  pages =         {59--67},
  title =         {The use of spatial relations in referring expression
                   generation},
  year =          {2008},
}

@inproceedings{Viethen:2011aa,
  author =        {Viethen, Jette and Dale, Robert and Guhe, Markus},
  booktitle =     {Proceedings of the Conference on Empirical Methods in
                   Natural Language Processing},
  organization =  {Association for Computational Linguistics},
  pages =         {1158--1167},
  title =         {Generating subsequent reference in shared visual
                   scenes: Computation vs. re-use},
  year =          {2011},
}

@inproceedings{Viethen:2011ab,
  author =        {Viethen, Jette and Dale, Robert and Guhe, Markus},
  booktitle =     {Proceedings of the 13th European Workshop on Natural
                   Language Generation},
  organization =  {Association for Computational Linguistics},
  pages =         {44--52},
  title =         {The impact of visual context on the content of
                   referring expressions},
  year =          {2011},
}

@article{Wang:2016aa,
  author =        {Wang, Sida I and Liang, Percy and
                   Manning, Christopher D},
  journal =       {arXiv preprint arXiv:1606.02447},
  title =         {Learning Language Games through Interaction},
  year =          {2016},
}

@article{Winograd:1972,
  author =        {Winograd, Terry},
  journal =       {Cognitive Psychology},
  number =        {1},
  publisher =     {Edinburgh University Press},
  title =         {Understanding Natural Language},
  volume =        {3},
  year =          {1972},
}

@book{Winograd:1976,
  author =        {Winograd, Terry},
  publisher =     {Edinburgh University Press},
  title =         {Understanding Natural Language},
  year =          {1976},
}

@article{Xu:2015aa,
  author =        {Xu, Kelvin and Ba, Jimmy and Kiros, Ryan and
                   Courville, Aaron and Salakhutdinov, Ruslan and
                   Zemel, Richard and Bengio, Yoshua},
  journal =       {arXiv},
  month =         {February 11},
  pages =         {1--22},
  title =         {Show, attend and tell: Neural image caption
                   generation with visual attention},
  volume =        {arXiv:1502.03044 [cs.LG]},
  year =          {2015},
}

@article{Xu:2018aa,
  author =        {Xiaofeng Xu and Ivor W. Tsang and Chuancai Liu},
  journal =       {arXiv},
  title =         {Zero-shot Learning with Complementary Attributes},
  volume =        {arXiv:1804.06505 [cs.CV]},
  year =          {2018},
  url =           {http://arxiv.org/abs/1804.06505},
}

@article{Yang:2014aa,
  author =        {Yang, Hee-Deok},
  journal =       {Sensors},
  number =        {1},
  pages =         {135--147},
  publisher =     {Multidisciplinary Digital Publishing Institute},
  title =         {Sign language recognition with the kinect sensor
                   based on conditional random fields},
  volume =        {15},
  year =          {2014},
}

@article{Yuan:2016aa,
  author =        {Yuan, Lei and Uttal, David and Franconeri, Steven},
  journal =       {PloS one},
  month =         {October},
  number =        {10},
  pages =         {1--22},
  publisher =     {Public Library of Science},
  title =         {Are categorical spatial relations encoded by shifting
                   visual attention between objects?},
  volume =        {11},
  year =          {2016},
  url =           {https://doi.org/10.1371/journal.pone.0163141},
}

@article{Zaslavsky:2018aa,
  author =        {Zaslavsky, Noga and Kemp, Charles and Regier, Terry and
                   Tishby, Naftali},
  journal =       {Proceedings of the National Academy of Sciences},
  number =        {31},
  pages =         {7937--7942},
  publisher =     {National Acad Sciences},
  title =         {Efficient compression in color naming and its
                   evolution},
  volume =        {115},
  year =          {2018},
}

@article{Zaslavsky:2019aa,
  author =        {Noga Zaslavsky and Terry Regier and Naftali Tishby and
                   Charles Kemp},
  journal =       {arXiv},
  title =         {Semantic categories of artifacts and animals reflect
                   efficient coding},
  volume =        {arXiv:1905.04562 [cs.CL]},
  year =          {2019},
  url =           {https://arxiv.org/abs/1905.04562},
}

@article{Zender:2008,
  author =        {Zender, Hendrik and {Mart\'{\i}nez-Mozos}, \'{O}scar and
                   Jensfelt, Patric and Kruijff, Geert-Jan M. and
                   Burgard, Wolfram},
  journal =       {Robotics and Autonomous Systems},
  month =         {June},
  note =          {Special issue ``From sensors to human spatial
                   concepts''},
  number =        {6},
  pages =         {493--502},
  title =         {Conceptual Spatial Representations for Indoor Mobile
                   Robots},
  volume =        {56},
  year =          {2008},
}

@article{Zender:2012uq,
  address =       {Los Alamitos, CA, USA},
  author =        {H. Zender and M. Janicek and Geert-Jan Kruijff},
  journal =       {IEEE Intelligent Systems},
  number =        {2},
  pages =         {27--35},
  publisher =     {IEEE Computer Society},
  title =         {Situated communication for joint activity in
                   human-robot teams},
  volume =        {27},
  year =          {2012},
  doi =               {http://doi.ieeecomputersociety.org/10.1109/MIS.2012.8},
  issn =          {1541-1672},
}

@inproceedings{Zhang:2019aa,
  author =        {Zhang, Ji and Kalantidis, Yannis and Rohrbach, Marcus and
                   Paluri, Manohar and Elgammal, Ahmed and
                   Elhoseiny, Mohamed},
  booktitle =     {Proceedings of the AAAI Conference on Artificial
                   Intelligence},
  pages =         {9185--9194},
  title =         {Large-scale visual relationship understanding},
  volume =        {33},
  year =          {2019},
  url =           {https://www.aaai.org/ojs/index.php/AAAI/article/view/4953/},
}

@inproceedings{Zhang:2019ab,
  author =        {Z. {Zhang} and Y. {Wang} and Q. {Wu} and F. {Chen}},
  booktitle =     {2019 International Joint Conference on Neural
                   Networks (IJCNN)},
  month =         {July},
  pages =         {1--8},
  title =         {Visual Relationship Attention for Image Captioning},
  year =          {2019},
  abstract =      {Visual attention mechanisms have been broadly used by
                   image captioning models to attend to related visual
                   information dynamically, allowing fine-grained image
                   understanding and reasoning. However, they are only
                   designed to discover the region-level alignment
                   between visual features and the language feature. The
                   exploration of higher-level visual relationship
                   information between image regions, which is rarely
                   researched in recent works, is beyond their
                   capabilities. To fill this gap, we propose a novel
                   visual relationship attention model based on the
                   parallel attention mechanism under the learnt spatial
                   constraints. It can extract relationship information
                   from visual regions and language and then achieve the
                   relationship-level alignment between them. Using
                   combined visual relationship attention and visual
                   region attention to attend to related visual
                   relationships and regions respectively, our image
                   captioning model can achieve state-of-the-art
                   performances on the MSCOCO dataset. Both quantitative
                   analysis and qualitative analysis demonstrate that
                   our novel visual relationship attention model can
                   capture related visual relationship and further
                   improve the caption quality.},
  doi =           {10.1109/IJCNN.2019.8851832},
  url =           {https://ieeexplore.ieee.org/abstract/document/8851832},
}

